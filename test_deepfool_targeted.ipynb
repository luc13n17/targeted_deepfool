{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from deepfool import deepfool\n",
    "from deepfool_targeted import deepfool_targeted\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import copy\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import json\n",
    "# import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, check the `deepfool_targeted.py` file to see if the needed function is commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select image and the target label. Here the target variable stores the index of the label from \" synset_words.txt \"\n",
    "im_orig = Image.open( 'test_im1.jpg' )\n",
    "target = 298"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ResNet50 (ImageNet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet50( pretrained=True )\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "\n",
    "mean = [ 0.485, 0.456, 0.406 ]\n",
    "std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "\n",
    "im = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean,\n",
    "                         std = std)])(im_orig)\n",
    "\n",
    "# r: Total perturbation\n",
    "# loop_i: Number of iterations required to fool the network\n",
    "# label_orig: Original label of the image\n",
    "# label_pert: Target label\n",
    "# pert_image: Image after addition of perturbation\n",
    "# confidences: A list containing confidence score for every iterations\n",
    "r, loop_i, label_orig, label_pert, pert_image, confidences = deepfool_targeted(im, net, target)\n",
    "\n",
    "labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "print(\"Original label = \", str_label_orig)\n",
    "print(\"Perturbed label = \", str_label_pert)\n",
    "\n",
    "def clip_tensor(A, minv, maxv):\n",
    "    A = torch.max(A, minv*torch.ones(A.shape))\n",
    "    A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "    return A\n",
    "\n",
    "clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "transforms.Lambda(clip),\n",
    "transforms.ToPILImage(),\n",
    "transforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image, perturbation, perturbed image\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "\n",
    "# Display the original image with its label\n",
    "axs[0].imshow(im)\n",
    "axs[0].set_title(str_label_orig)\n",
    "\n",
    "\n",
    "axs[1].imshow(tf(pert_image.cpu()[0]))\n",
    "axs[1].set_title(str_label_pert)\n",
    "\n",
    "# Display only the perturbations\n",
    "r = np.squeeze(r)  # remove the first dimension\n",
    "r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "\n",
    "# Display the perturbed image with its label\n",
    "axs[2].imshow(np.abs(r))\n",
    "axs[2].set_title('Perturbation')\n",
    "\n",
    "# # Display only the perturbations amplified\n",
    "# r = np.squeeze(r)\n",
    "# r = np.transpose(r, (1, 2, 0))\n",
    "# amplification_factor = 5\n",
    "# r_amplified = amplification_factor * r\n",
    "# r_amplified = np.clip(r_amplified, 0, 1)\n",
    "# axs[2].imshow(r_amplified)\n",
    "# axs[2].set_title('Perturbation Mask (Amplified)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confidence scores for perturbed image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(confidences)), confidences, color='blue', linestyle='--')\n",
    "plt.title(\"Change in Target Class Confidence\") \n",
    "plt.xlabel(\"Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Target Class Confidence\", fontsize=14)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **AlexNet (ImageNet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.alexnet(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "mean = [ 0.485, 0.456, 0.406 ]\n",
    "std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "\n",
    "\n",
    "im = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean,\n",
    "                         std = std)])(im_orig)\n",
    "\n",
    "# r: Total perturbation\n",
    "# loop_i: Number of iterations required to fool the network\n",
    "# label_orig: Original label of the image\n",
    "# label_pert: Target label\n",
    "# pert_image: Image after addition of perturbation\n",
    "# confidences: A list containing confidence score for every iterations\n",
    "r, loop_i, label_orig, label_pert, pert_image, confidences = deepfool_targeted(im, net, target)\n",
    "\n",
    "labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "print(\"Original label = \", str_label_orig)\n",
    "print(\"Perturbed label = \", str_label_pert)\n",
    "\n",
    "def clip_tensor(A, minv, maxv):\n",
    "    A = torch.max(A, minv*torch.ones(A.shape))\n",
    "    A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "    return A\n",
    "\n",
    "clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "transforms.Lambda(clip),\n",
    "transforms.ToPILImage(),\n",
    "transforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image, perturbation, perturbed image\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "\n",
    "# Display the original image with its label\n",
    "axs[0].imshow(im)\n",
    "axs[0].set_title(str_label_orig)\n",
    "\n",
    "\n",
    "axs[1].imshow(tf(pert_image.cpu()[0]))\n",
    "axs[1].set_title(str_label_pert)\n",
    "\n",
    "# Display only the perturbations\n",
    "r = np.squeeze(r)  # remove the first dimension\n",
    "r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "\n",
    "# Display the perturbed image with its label\n",
    "axs[2].imshow(np.abs(r))\n",
    "axs[2].set_title('Perturbation')\n",
    "\n",
    "# # Display only the perturbations amplified\n",
    "# r = np.squeeze(r)\n",
    "# r = np.transpose(r, (1, 2, 0))\n",
    "# amplification_factor = 5\n",
    "# r_amplified = amplification_factor * r\n",
    "# r_amplified = np.clip(r_amplified, 0, 1)\n",
    "# axs[2].imshow(r_amplified)\n",
    "# axs[2].set_title('Perturbation Mask (Amplified)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confidence scores for perturbed image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(confidences)), confidences, color='blue', linestyle='--')\n",
    "plt.title(\"Change in Target Class Confidence\") \n",
    "plt.xlabel(\"Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Target Class Confidence\", fontsize=14)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EfficientNetV2 (ImageNet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.efficientnet_v2_s(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "mean = [ 0.485, 0.456, 0.406 ]\n",
    "std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "\n",
    "\n",
    "im = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean,\n",
    "                         std = std)])(im_orig)\n",
    "\n",
    "# r: Total perturbation\n",
    "# loop_i: Number of iterations required to fool the network\n",
    "# label_orig: Original label of the image\n",
    "# label_pert: Target label\n",
    "# pert_image: Image after addition of perturbation\n",
    "# confidences: A list containing confidence score for every iterations\n",
    "r, loop_i, label_orig, label_pert, pert_image, confidences = deepfool_targeted(im, net, target)\n",
    "\n",
    "labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "print(\"Original label = \", str_label_orig)\n",
    "print(\"Perturbed label = \", str_label_pert)\n",
    "\n",
    "def clip_tensor(A, minv, maxv):\n",
    "    A = torch.max(A, minv*torch.ones(A.shape))\n",
    "    A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "    return A\n",
    "\n",
    "clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "transforms.Lambda(clip),\n",
    "transforms.ToPILImage(),\n",
    "transforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image, perturbation, perturbed image\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "\n",
    "# Display the original image with its label\n",
    "axs[0].imshow(im)\n",
    "axs[0].set_title(str_label_orig)\n",
    "\n",
    "\n",
    "axs[1].imshow(tf(pert_image.cpu()[0]))\n",
    "axs[1].set_title(str_label_pert)\n",
    "\n",
    "# Display only the perturbations\n",
    "r = np.squeeze(r)  # remove the first dimension\n",
    "r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "\n",
    "# Display the perturbed image with its label\n",
    "axs[2].imshow(np.abs(r))\n",
    "axs[2].set_title('Perturbation')\n",
    "\n",
    "# # Display only the perturbations amplified\n",
    "# r = np.squeeze(r)\n",
    "# r = np.transpose(r, (1, 2, 0))\n",
    "# amplification_factor = 5\n",
    "# r_amplified = amplification_factor * r\n",
    "# r_amplified = np.clip(r_amplified, 0, 1)\n",
    "# axs[2].imshow(r_amplified)\n",
    "# axs[2].set_title('Perturbation Mask (Amplified)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confidence scores for perturbed image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(confidences)), confidences, color='blue', linestyle='--')\n",
    "plt.title(\"Change in Target Class Confidence\") \n",
    "plt.xlabel(\"Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Target Class Confidence\", fontsize=14)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GoogleNet (ImageNet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.googlenet(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "mean = [ 0.485, 0.456, 0.406 ]\n",
    "std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "\n",
    "\n",
    "im = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean,\n",
    "                         std = std)])(im_orig)\n",
    "\n",
    "# r: Total perturbation\n",
    "# loop_i: Number of iterations required to fool the network\n",
    "# label_orig: Original label of the image\n",
    "# label_pert: Target label\n",
    "# pert_image: Image after addition of perturbation\n",
    "# confidences: A list containing confidence score for every iterations\n",
    "r, loop_i, label_orig, label_pert, pert_image, confidences = deepfool_targeted(im, net, target)\n",
    "\n",
    "labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "print(\"Original label = \", str_label_orig)\n",
    "print(\"Perturbed label = \", str_label_pert)\n",
    "\n",
    "def clip_tensor(A, minv, maxv):\n",
    "    A = torch.max(A, minv*torch.ones(A.shape))\n",
    "    A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "    return A\n",
    "\n",
    "clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "transforms.Lambda(clip),\n",
    "transforms.ToPILImage(),\n",
    "transforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image, perturbation, perturbed image\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "\n",
    "# Display the original image with its label\n",
    "axs[0].imshow(im)\n",
    "axs[0].set_title(str_label_orig)\n",
    "\n",
    "\n",
    "axs[1].imshow(tf(pert_image.cpu()[0]))\n",
    "axs[1].set_title(str_label_pert)\n",
    "\n",
    "# Display only the perturbations\n",
    "r = np.squeeze(r)  # remove the first dimension\n",
    "r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "\n",
    "# Display the perturbed image with its label\n",
    "axs[2].imshow(np.abs(r))\n",
    "axs[2].set_title('Perturbation')\n",
    "\n",
    "# # Display only the perturbations amplified\n",
    "# r = np.squeeze(r)\n",
    "# r = np.transpose(r, (1, 2, 0))\n",
    "# amplification_factor = 5\n",
    "# r_amplified = amplification_factor * r\n",
    "# r_amplified = np.clip(r_amplified, 0, 1)\n",
    "# axs[2].imshow(r_amplified)\n",
    "# axs[2].set_title('Perturbation Mask (Amplified)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confidence scores for perturbed image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(confidences)), confidences, color='blue', linestyle='--')\n",
    "plt.title(\"Change in Target Class Confidence\") \n",
    "plt.xlabel(\"Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Target Class Confidence\", fontsize=14)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **InceptionV3 (ImageNet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.inception_v3(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "mean = [ 0.485, 0.456, 0.406 ]\n",
    "std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "\n",
    "\n",
    "im = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean,\n",
    "                         std = std)])(im_orig)\n",
    "\n",
    "# r: Total perturbation\n",
    "# loop_i: Number of iterations required to fool the network\n",
    "# label_orig: Original label of the image\n",
    "# label_pert: Target label\n",
    "# pert_image: Image after addition of perturbation\n",
    "# confidences: A list containing confidence score for every iterations\n",
    "r, loop_i, label_orig, label_pert, pert_image, confidences = deepfool_targeted(im, net, target)\n",
    "\n",
    "labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "print(\"Original label = \", str_label_orig)\n",
    "print(\"Perturbed label = \", str_label_pert)\n",
    "\n",
    "def clip_tensor(A, minv, maxv):\n",
    "    A = torch.max(A, minv*torch.ones(A.shape))\n",
    "    A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "    return A\n",
    "\n",
    "clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "transforms.Lambda(clip),\n",
    "transforms.ToPILImage(),\n",
    "transforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image, perturbation, perturbed image\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "\n",
    "# Display the original image with its label\n",
    "axs[0].imshow(im)\n",
    "axs[0].set_title(str_label_orig)\n",
    "\n",
    "\n",
    "axs[1].imshow(tf(pert_image.cpu()[0]))\n",
    "axs[1].set_title(str_label_pert)\n",
    "\n",
    "# Display only the perturbations\n",
    "r = np.squeeze(r)  # remove the first dimension\n",
    "r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "\n",
    "# Display the perturbed image with its label\n",
    "axs[2].imshow(np.abs(r))\n",
    "axs[2].set_title('Perturbation')\n",
    "\n",
    "# # Display only the perturbations amplified\n",
    "# r = np.squeeze(r)\n",
    "# r = np.transpose(r, (1, 2, 0))\n",
    "# amplification_factor = 5\n",
    "# r_amplified = amplification_factor * r\n",
    "# r_amplified = np.clip(r_amplified, 0, 1)\n",
    "# axs[2].imshow(r_amplified)\n",
    "# axs[2].set_title('Perturbation Mask (Amplified)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confidence scores for perturbed image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(confidences)), confidences, color='blue', linestyle='--')\n",
    "plt.title(\"Change in Target Class Confidence\") \n",
    "plt.xlabel(\"Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Target Class Confidence\", fontsize=14)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VisionTransformer (ImageNet)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "mean = [ 0.485, 0.456, 0.406 ]\n",
    "std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "\n",
    "\n",
    "im = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean,\n",
    "                         std = std)])(im_orig)\n",
    "\n",
    "# r: Total perturbation\n",
    "# loop_i: Number of iterations required to fool the network\n",
    "# label_orig: Original label of the image\n",
    "# label_pert: Target label\n",
    "# pert_image: Image after addition of perturbation\n",
    "# confidences: A list containing confidence score for every iterations\n",
    "r, loop_i, label_orig, label_pert, pert_image, confidences = deepfool_targeted(im, net, target)\n",
    "\n",
    "labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "print(\"Original label = \", str_label_orig)\n",
    "print(\"Perturbed label = \", str_label_pert)\n",
    "\n",
    "def clip_tensor(A, minv, maxv):\n",
    "    A = torch.max(A, minv*torch.ones(A.shape))\n",
    "    A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "    return A\n",
    "\n",
    "clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "transforms.Lambda(clip),\n",
    "transforms.ToPILImage(),\n",
    "transforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image, perturbation, perturbed image\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "\n",
    "# Display the original image with its label\n",
    "axs[0].imshow(im)\n",
    "axs[0].set_title(str_label_orig)\n",
    "\n",
    "\n",
    "axs[1].imshow(tf(pert_image.cpu()[0]))\n",
    "axs[1].set_title(str_label_pert)\n",
    "\n",
    "# Display only the perturbations\n",
    "r = np.squeeze(r)  # remove the first dimension\n",
    "r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "\n",
    "# Display the perturbed image with its label\n",
    "axs[2].imshow(np.abs(r))\n",
    "axs[2].set_title('Perturbation')\n",
    "\n",
    "# # Display only the perturbations amplified\n",
    "# r = np.squeeze(r)\n",
    "# r = np.transpose(r, (1, 2, 0))\n",
    "# amplification_factor = 5\n",
    "# r_amplified = amplification_factor * r\n",
    "# r_amplified = np.clip(r_amplified, 0, 1)\n",
    "# axs[2].imshow(r_amplified)\n",
    "# axs[2].set_title('Perturbation Mask (Amplified)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confidence scores for perturbed image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(confidences)), confidences, color='blue', linestyle='--')\n",
    "plt.title(\"Change in Target Class Confidence\") \n",
    "plt.xlabel(\"Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Target Class Confidence\", fontsize=14)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Run test on a single image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [models.resnet50(pretrained=True),\n",
    "               models.alexnet(pretrained=True),\n",
    "               models.efficientnet_v2_s(pretrained=True),\n",
    "               models.googlenet(pretrained=True),\n",
    "               models.inception_v3(pretrained=True),\n",
    "               models.vit_b_16(pretrained=True)]\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for i, net in enumerate(models_list):\n",
    "    # Switch to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    im_orig = Image.open('./images/test_images/test_im4.jpeg')\n",
    "    target = 413\n",
    "\n",
    "    mean = [ 0.485, 0.456, 0.406 ]\n",
    "    std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "    im = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = mean,\n",
    "                             std = std)])(im_orig)\n",
    "\n",
    "    r, loop_i, label_orig, label_pert, pert_image, confidences = deepfool_targeted(im, net, target)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    results_dict[i] = {\n",
    "        \"r\": r,\n",
    "        \"loop_i\": loop_i,\n",
    "        \"label_orig\": label_orig,\n",
    "        \"label_pert\": label_pert,\n",
    "        \"pert_image\": pert_image,\n",
    "        \"confidences\": confidences\n",
    "    }\n",
    "\n",
    "    labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "    str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "    str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "    print(\"Original label for model {} = {}\".format(i+1, str_label_orig))\n",
    "    print(\"Perturbed label for model {} = {}\".format(i+1, str_label_pert))\n",
    "\n",
    "    def clip_tensor(A, minv, maxv):\n",
    "        A = torch.max(A, minv*torch.ones(A.shape))\n",
    "        A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "        return A\n",
    "\n",
    "    clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "    tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "    transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "    transforms.Lambda(clip),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(224)])\n",
    "    \n",
    "    ## Run the following code if you want to save the images ##\n",
    "\n",
    "    # Save the original image\n",
    "    # Convert the tensor to a numpy array\n",
    "    im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    # # Denormalize the image\n",
    "    im = std * im + mean\n",
    "    im = np.clip(im, 0, 1)\n",
    "    # # Convert the NumPy array to a PIL image object\n",
    "    im_pil = Image.fromarray(np.uint8(im * 255))\n",
    "\n",
    "    # # Save the original image\n",
    "    im_pil.save(f\"original_{i}.jpg\")\n",
    "\n",
    "    # Save the perturbed image\n",
    "    pert_image = tf(pert_image.cpu()[0])\n",
    "    pert_image.save(f\"perturbed_{i}.jpg\")\n",
    "\n",
    "    # # Save perturbations\n",
    "    # r = np.squeeze(r)  # remove the first dimension\n",
    "    # r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "    # r = np.abs(r)\n",
    "    # r_pil = Image.fromarray(np.uint8(r * 255))\n",
    "    # r_pil.save(f\"perturbation_{i}.jpg\")\n",
    "\n",
    "    # Save perturbations amplified\n",
    "    r = np.squeeze(r)\n",
    "    r = np.transpose(r, (1, 2, 0))\n",
    "    amplification_factor = 20\n",
    "    r_amplified = amplification_factor * r\n",
    "    r_amplified = np.abs(r_amplified)\n",
    "    r_pil = Image.fromarray(np.uint8(r_amplified * 255))\n",
    "    r_pil.save(f\"perturbation_amplified{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# Plot the confidence scores for each perturbed image\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "for i in range(6):\n",
    "    confidences = results_dict[i][\"confidences\"]\n",
    "    color = ['blue', 'red', 'green', 'orange', 'purple', 'gray'][i % 6]\n",
    "    linestyle = ['--', ':', '-.', '--', ':', '-.'][i % 6]\n",
    "    label = [\"ResNet50\", \"AlexNet\", \"EfficientNetV2\", \"GoogLeNet\", \"InceptionV3\", \"Vision Transformer\"][i % 6]\n",
    "    plt.plot(range(len(confidences)), confidences, color=color, linestyle=linestyle, label=label)\n",
    "\n",
    "plt.title(\"Change in Target Class Confidence ({} to {})\".format(str_label_orig, str_label_pert)) \n",
    "plt.xlabel(\"Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Target Class Confidence\", fontsize=14)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12, loc='center left', bbox_to_anchor=(0.6, 0.8))\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DeepFool vs Targeted DeepFool**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DeepFool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_orig = Image.open('test_im1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet50(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "mean = [ 0.485, 0.456, 0.406 ]\n",
    "std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "\n",
    "im = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean,\n",
    "                         std = std)])(im_orig)\n",
    "\n",
    "\n",
    "r, loop_i, label_orig, label_pert, pert_image = deepfool(im, net)\n",
    "\n",
    "labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "print(\"Original label = \", str_label_orig)\n",
    "print(\"Perturbed label = \", str_label_pert)\n",
    "\n",
    "def clip_tensor(A, minv, maxv):\n",
    "    A = torch.max(A, minv*torch.ones(A.shape))\n",
    "    A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "    return A\n",
    "\n",
    "clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "transforms.Lambda(clip),\n",
    "transforms.ToPILImage(),\n",
    "transforms.CenterCrop(224)])\n",
    "\n",
    "## Run following codes if you want to save the images ##\n",
    "\n",
    "# Save the original image\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "# # Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "# # Convert the NumPy array to a PIL image object\n",
    "im_pil = Image.fromarray(np.uint8(im * 255))\n",
    "# # Save the original image\n",
    "im_pil.save(f\"original.jpg\")\n",
    "\n",
    "# Save the perturbed image\n",
    "pert_image = tf(pert_image.cpu()[0])\n",
    "pert_image.save(f\"perturbed.jpg\")\n",
    "\n",
    "# # Save perturbations\n",
    "# r = np.squeeze(r)  # remove the first dimension\n",
    "# r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "# r = np.abs(r)\n",
    "# r_pil = Image.fromarray(np.uint8(r * 255))\n",
    "# r_pil.save(f\"perturbation.jpg\")\n",
    "\n",
    "# Save amplified perturbations\n",
    "r = np.squeeze(r)\n",
    "r = np.transpose(r, (1, 2, 0))\n",
    "amplification_factor = 20\n",
    "r_amplified = amplification_factor * r\n",
    "r_amplified = np.abs(r_amplified)\n",
    "r_pil = Image.fromarray(np.uint8(r_amplified * 255))\n",
    "r_pil.save(f\"perturbation(amplified).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "# Display the original image with its label\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "\n",
    "axs[0].imshow(im)\n",
    "axs[0].set_title(str_label_orig)\n",
    "\n",
    "# Display the perturbed image with its label\n",
    "axs[1].imshow(tf(pert_image.cpu()[0]))\n",
    "axs[1].set_title(str_label_pert)\n",
    "\n",
    "# # Display only the perturbations\n",
    "# r = np.squeeze(r)  # remove the first dimension\n",
    "# r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "# axs[2].imshow(np.abs(r))\n",
    "# axs[2].set_title('Perturbation')\n",
    "\n",
    "# Display only the perturbations amplified\n",
    "r = np.squeeze(r)\n",
    "r = np.transpose(r, (1, 2, 0))\n",
    "amplification_factor = 10\n",
    "r_amplified = amplification_factor * r\n",
    "r_amplified = np.clip(r_amplified, 0, 1)\n",
    "axs[2].imshow(r_amplified)\n",
    "axs[2].set_title('Perturbation Mask (Amplified)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Targeted DeepFool**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_orig = Image.open('test_im1.jpg')\n",
    "target = 298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet50(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "mean = [ 0.485, 0.456, 0.406 ]\n",
    "std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "\n",
    "im = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = mean,\n",
    "                         std = std)])(im_orig)\n",
    "\n",
    "r, loop_i, label_orig, label_pert, pert_image, confidences = deepfool_targeted(im, net, target)\n",
    "\n",
    "labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "\n",
    "str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "print(\"Original label = \", str_label_orig)\n",
    "print(\"Perturbed label = \", str_label_pert)\n",
    "\n",
    "def clip_tensor(A, minv, maxv):\n",
    "    A = torch.max(A, minv*torch.ones(A.shape))\n",
    "    A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "    return A\n",
    "\n",
    "clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "transforms.Lambda(clip),\n",
    "transforms.ToPILImage(),\n",
    "transforms.CenterCrop(224)])\n",
    "\n",
    "## Run following codes if you want to save the images ##\n",
    "\n",
    "# Save the original image\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "# # Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "# # Convert the NumPy array to a PIL image object\n",
    "im_pil = Image.fromarray(np.uint8(im * 255))\n",
    "# # Save the original image\n",
    "im_pil.save(f\"original.jpg\")\n",
    "# Save the perturbed image\n",
    "pert_image = tf(pert_image.cpu()[0])\n",
    "pert_image.save(f\"perturbed.jpg\")\n",
    "\n",
    "# # Save perturbations\n",
    "# r = np.squeeze(r)  # remove the first dimension\n",
    "# r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "# r = np.abs(r)\n",
    "# r_pil = Image.fromarray(np.uint8(r * 255))\n",
    "# r_pil.save(f\"perturbation.jpg\")\n",
    "\n",
    "# Save perturbations amplified\n",
    "r = np.squeeze(r)\n",
    "r = np.transpose(r, (1, 2, 0))\n",
    "amplification_factor = 20\n",
    "r_amplified = amplification_factor * r\n",
    "r_amplified = np.abs(r_amplified)\n",
    "r_pil = Image.fromarray(np.uint8(r_amplified * 255))\n",
    "r_pil.save(f\"perturbation(amplified).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "# Display the original image with its label\n",
    "# Convert the tensor to a numpy array\n",
    "im = im.numpy().transpose((1, 2, 0))\n",
    "\n",
    "# Denormalize the image\n",
    "im = std * im + mean\n",
    "im = np.clip(im, 0, 1)\n",
    "\n",
    "axs[0].imshow(im)\n",
    "axs[0].set_title(str_label_orig)\n",
    "\n",
    "# Display the perturbed image with its label\n",
    "axs[1].imshow(tf(pert_image.cpu()[0]))\n",
    "axs[1].set_title(str_label_pert)\n",
    "\n",
    "# # Display only the perturbations\n",
    "# r = np.squeeze(r)  # remove the first dimension\n",
    "# r = np.transpose(r, (1, 2, 0))  # transpose the dimensions to (H, W, 3)\n",
    "# axs[2].imshow(np.abs(r))\n",
    "# axs[2].set_title('Perturbation')\n",
    "\n",
    "# Display only the perturbations amplified\n",
    "r = np.squeeze(r)\n",
    "r = np.transpose(r, (1, 2, 0))\n",
    "amplification_factor = 10\n",
    "r_amplified = amplification_factor * r\n",
    "r_amplified = np.clip(r_amplified, 0, 1)\n",
    "axs[2].imshow(r_amplified)\n",
    "axs[2].set_title('Perturbation Mask (Amplified)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confidence scores for perturbed image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(confidences)), confidences, color='blue', linestyle='--')\n",
    "plt.title(\"Change in Target Class Confidence\") \n",
    "plt.xlabel(\"Iterations\", fontsize=14)\n",
    "plt.ylabel(\"Target Class Confidence\", fontsize=14)\n",
    "plt.xticks(fontsize=12) \n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

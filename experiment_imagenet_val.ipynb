{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**\n",
    "It is best to to create a virtual environment.\n",
    "\n",
    "We need to install the following libraries and dependencies<br>\n",
    "\n",
    "Run the following commands:<br>\n",
    "`pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`<br>\n",
    "`pip install torchmetrics`<br>\n",
    "`pip install matplotlib`<br>\n",
    "\n",
    "Download the **Imagenet2012 Validation dataset** and unzip it to the **\"data\"** directory.<br>\n",
    "\n",
    "You might run into a problem when running the first cell. The following error might show up: **\"cannot import name 'zero_gradients' from 'torch.autograd.gradcheck\"**<br>\n",
    "There will be a link at the end of the error, which should lead you to **gradcheck.py** file<br>\n",
    "Copy and paste the following code the **gradcheck.py** file\n",
    "\n",
    "```\n",
    "def zero_gradients(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.grad is not None:\n",
    "            x.grad.detach_()\n",
    "            x.grad.zero_()\n",
    "    elif isinstance(x, collections.abc.Iterable):\n",
    "        for elem in x:\n",
    "            zero_gradients(elem)\n",
    "```\n",
    "Save the gradcheck.py file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from deepfool_targeted import deepfool_targeted\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch as torch\n",
    "import copy\n",
    "from torch.autograd.gradcheck import zero_gradients\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import json\n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used models\n",
    "# models.resnet50(pretrained=True),\n",
    "# models.alexnet(pretrained=True),\n",
    "# models.efficientnet_v2_s(pretrained=True),\n",
    "# models.googlenet(pretrained=True),\n",
    "# models.inception_v3(pretrained=True),\n",
    "# models.vit_b_16(pretrained=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, check the `deepfool_targeted.py` file to see if the needed function is commented out."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "stats_dict = {}\n",
    "stats_dict_temp = {}\n",
    "\n",
    "# Storing true labels in an array\n",
    "filename = './data/LOC_val_solution/ILSVRC2012_val_true_label_indices.txt'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    true_labels = []\n",
    "    for line in file:\n",
    "        true_labels.append(int(line.strip()))\n",
    "\n",
    "net = models.resnet50(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "img_dir = './data/ILSVRC2012_img_val/'\n",
    "\n",
    "files = sorted(os.listdir(img_dir)) # Use this line if you want to run for the whole dataset\n",
    "\n",
    "# Select only the specific file names \n",
    "# [1000:] = start from 1000 and end at file end, \n",
    "# [:1000] = run from start till 1000th file\n",
    "# [65:36000] = run from 66th file to 36000th file\n",
    "\n",
    "# files = os.listdir(img_dir)[36287:] #Change the value inside [] according to need\n",
    "\n",
    "confidences_pert = []\n",
    "l2_norms = []\n",
    "ssim_scores = []\n",
    "index = 0\n",
    "iterations = []\n",
    "success = []\n",
    "\n",
    "for filename in files:\n",
    "    \n",
    "    # Create the 'perturbed_images' directory if it doesn't exist\n",
    "    if not os.path.exists('./data/perturbed_images_resnet50'):\n",
    "        os.makedirs('./data/perturbed_images_resnet50')\n",
    "\n",
    "    # Load the image\n",
    "    im_orig = Image.open(os.path.join(img_dir, filename)).convert('RGB')\n",
    "    # im_orig = Image.open(os.path.join(img_dir, filename))\n",
    "    # if im_orig.mode == 'L': # grayscale image\n",
    "    #     im_orig = im_orig.convert('RGB')\n",
    "\n",
    "    mean = [ 0.485, 0.456, 0.406 ]\n",
    "    std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "    # Apply the transformation pipeline\n",
    "    im = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = mean,\n",
    "                             std = std)])(im_orig)\n",
    "    \n",
    "    # randomly select target class\n",
    "    target = np.random.choice([i for i in range(1000) if i != true_labels[index]])\n",
    "\n",
    "    # Perform the targeted deepfool attack\n",
    "    r, loop_i, label_orig, label_pert, pert_image, confidences, confidence_pert, l2_norm, ssim_value, successful = deepfool_targeted(im, net, target)\n",
    "\n",
    "    confidences_pert.append(confidence_pert)\n",
    "    l2_norms.append(l2_norm)\n",
    "    ssim_scores.append(ssim_value)\n",
    "    iterations.append(loop_i)\n",
    "    success.append(successful)\n",
    "\n",
    "    labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "    \n",
    "    # Get the string label for the original and perturbed images\n",
    "    str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "    str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "    def clip_tensor(A, minv, maxv):\n",
    "        A = torch.max(A, minv*torch.ones(A.shape))\n",
    "        A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "        return A\n",
    "\n",
    "    clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "    tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "    transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "    transforms.Lambda(clip),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(224)])\n",
    "\n",
    "    # Clip the perturbed image and convert it back to a PIL image\n",
    "    pert_image = tf(pert_image.cpu()[0])\n",
    "\n",
    "    # Save the perturbed image\n",
    "    pert_image.save(os.path.join('./data/perturbed_images_resnet50', filename))\n",
    "    \n",
    "    # Print the labels of the original and perturbed images\n",
    "    print(f\"{filename}: Original label = {str_label_orig}, Perturbed label = {str_label_pert}\")\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "    # Add model name as key and corresponding values as values in stats_dict_temp\n",
    "    # Update test_dict with the new values for alexnet\n",
    "    # Check if 'resnet50' is already in stats_dict_temp\n",
    "    if 'resnet50' in stats_dict_temp:\n",
    "        # If it is, update the existing dictionary for 'resnet50'\n",
    "        stats_dict_temp['resnet50'].update({\n",
    "            'confidences_pert': stats_dict_temp['resnet50'].get('confidences_pert', []) + [confidence_pert],\n",
    "            'l2_norms': stats_dict_temp['resnet50'].get('l2_norms', []) + [l2_norm],\n",
    "            'ssim_scores': stats_dict_temp['resnet50'].get('ssim_scores', []) + [ssim_value],\n",
    "            'iterations': stats_dict_temp['resnet50'].get('iterations', []) + [loop_i],\n",
    "            'success': stats_dict_temp['resnet50'].get('success', []) + [successful]\n",
    "        })\n",
    "    else:\n",
    "        # If it isn't, add a new dictionary for 'resnet50'\n",
    "        stats_dict_temp['resnet50'] = {\n",
    "            'confidences_pert': [confidence_pert],\n",
    "            'l2_norms': [l2_norm],\n",
    "            'ssim_scores': [ssim_value],\n",
    "            'iterations': [loop_i],\n",
    "            'success': [successful]\n",
    "        }\n",
    "    \n",
    "    with open('./stats/resnet50_stats_temp.json', 'w') as f:\n",
    "        json.dump(stats_dict_temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time =  time.process_time() - start\n",
    "print(\"Total Time: \",time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model name as key and corresponding values as values in stats_dict\n",
    "stats_dict.update({'resnet50': {\n",
    "                            'confidences_pert': confidences_pert,\n",
    "                            'l2_norms': l2_norms,\n",
    "                            'ssim_scores': ssim_scores,\n",
    "                            'iterations': iterations,\n",
    "                            'success': success\n",
    "                            }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dictionary to file\n",
    "with open('./stats/resnet50_stats.json', 'w') as f:\n",
    "    json.dump(stats_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary from file\n",
    "with open('./stats/resnet50_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"resnet50\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(stats['confidences_pert'])/len(stats['confidences_pert']):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(stats['l2_norms'])/len(stats['l2_norms']):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(stats['ssim_scores'])/len(stats['ssim_scores']):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {sum(stats['iterations'])/len(stats['iterations']):.4f}\")\n",
    "print(f\"Mean success for {model_name}: {sum(stats['success'])/len(stats['success']):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows results after excluding failed attacks\n",
    "\n",
    "with open('./stats/resnet50_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"resnet50\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "success_indices = [i for i, success in enumerate(stats['success']) if success]\n",
    "\n",
    "success_confidences = [stats['confidences_pert'][i] for i in success_indices]\n",
    "success_l2_norms = [stats['l2_norms'][i] for i in success_indices]\n",
    "success_ssim_scores = [stats['ssim_scores'][i] for i in success_indices]\n",
    "success_iterations = [stats['iterations'][i] for i in success_indices]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(success_confidences) / len(success_confidences):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(success_l2_norms) / len(success_l2_norms):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(success_ssim_scores) / len(success_ssim_scores):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {math.ceil(sum(success_iterations) / len(success_iterations))}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AlexNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "stats_dict = {}\n",
    "stats_dict_temp = {}\n",
    "# Storing true labels in an array\n",
    "filename = './data/LOC_val_solution/ILSVRC2012_val_true_label_indices.txt'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    true_labels = []\n",
    "    for line in file:\n",
    "        true_labels.append(int(line.strip()))\n",
    "\n",
    "net = models.alexnet(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "img_dir = './data/ILSVRC2012_img_val/'\n",
    "\n",
    "files = sorted(os.listdir(img_dir)) # Use this line if you want to run for the whole dataset\n",
    "\n",
    "# Select only the specific file names \n",
    "# [1000:] = start from 1000 and end at file end, \n",
    "# [:1000] = run from start till 1000th file\n",
    "# [65:36000] = run from 66th file to 36000th file\n",
    "\n",
    "# files = os.listdir(img_dir)[36287:] #Change the value inside [] according to need\n",
    "\n",
    "confidences_pert = []\n",
    "l2_norms = []\n",
    "ssim_scores = []\n",
    "index = 0\n",
    "iterations = []\n",
    "success = []\n",
    "\n",
    "for filename in files:\n",
    "    \n",
    "    # Create the 'perturbed_images' directory if it doesn't exist\n",
    "    if not os.path.exists('./data/perturbed_images_alexnet'):\n",
    "        os.makedirs('./data/perturbed_images_alexnet')\n",
    "\n",
    "    # Load the image\n",
    "    im_orig = Image.open(os.path.join(img_dir, filename)).convert('RGB')\n",
    "    # im_orig = Image.open(os.path.join(img_dir, filename))\n",
    "    # if im_orig.mode == 'L': # grayscale image\n",
    "    #     im_orig = im_orig.convert('RGB')\n",
    "\n",
    "    mean = [ 0.485, 0.456, 0.406 ]\n",
    "    std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "    # Apply the transformation pipeline\n",
    "    im = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = mean,\n",
    "                             std = std)])(im_orig)\n",
    "    \n",
    "    # randomly select target class\n",
    "    target = np.random.choice([i for i in range(1000) if i != true_labels[index]])\n",
    "\n",
    "    # Perform the targeted deepfool attack\n",
    "    r, loop_i, label_orig, label_pert, pert_image, confidences, confidence_pert, l2_norm, ssim_value, successful = deepfool_targeted(im, net, target)\n",
    "\n",
    "    confidences_pert.append(confidence_pert)\n",
    "    l2_norms.append(l2_norm)\n",
    "    ssim_scores.append(ssim_value)\n",
    "    iterations.append(loop_i)\n",
    "    success.append(successful)\n",
    "\n",
    "    labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "    \n",
    "    # Get the string label for the original and perturbed images\n",
    "    str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "    str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "    def clip_tensor(A, minv, maxv):\n",
    "        A = torch.max(A, minv*torch.ones(A.shape))\n",
    "        A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "        return A\n",
    "\n",
    "    clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "    tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "    transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "    transforms.Lambda(clip),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(224)])\n",
    "\n",
    "    # Clip the perturbed image and convert it back to a PIL image\n",
    "    pert_image = tf(pert_image.cpu()[0])\n",
    "\n",
    "    # Save the perturbed image\n",
    "    pert_image.save(os.path.join('./data/perturbed_images_alexnet', filename))\n",
    "    \n",
    "    # Print the labels of the original and perturbed images\n",
    "    print(f\"{filename}: Original label = {str_label_orig}, Perturbed label = {str_label_pert}\")\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "    # Add model name as key and corresponding values as values in stats_dict_temp\n",
    "    # Update test_dict with the new values for alexnet\n",
    "    # Check if 'alexnet' is already in stats_dict_temp\n",
    "    if 'alexnet' in stats_dict_temp:\n",
    "        # If it is, update the existing dictionary for 'alexnet'\n",
    "        stats_dict_temp['alexnet'].update({\n",
    "            'confidences_pert': stats_dict_temp['alexnet'].get('confidences_pert', []) + [confidence_pert],\n",
    "            'l2_norms': stats_dict_temp['alexnet'].get('l2_norms', []) + [l2_norm],\n",
    "            'ssim_scores': stats_dict_temp['alexnet'].get('ssim_scores', []) + [ssim_value],\n",
    "            'iterations': stats_dict_temp['alexnet'].get('iterations', []) + [loop_i],\n",
    "            'success': stats_dict_temp['alexnet'].get('success', []) + [successful]\n",
    "        })\n",
    "    else:\n",
    "        # If it isn't, add a new dictionary for 'alexnet'\n",
    "        stats_dict_temp['alexnet'] = {\n",
    "            'confidences_pert': [confidence_pert],\n",
    "            'l2_norms': [l2_norm],\n",
    "            'ssim_scores': [ssim_value],\n",
    "            'iterations': [loop_i],\n",
    "            'success': [successful]\n",
    "        }\n",
    "    \n",
    "    with open('./stats/alexnet_stats_temp.json', 'w') as f:\n",
    "        json.dump(stats_dict_temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time =  time.process_time() - start\n",
    "print(\"Total Time: \",time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model name as key and corresponding values as values in stats_dict\n",
    "stats_dict.update({'alexnet': {\n",
    "                            'confidences_pert': confidences_pert,\n",
    "                            'l2_norms': l2_norms,\n",
    "                            'ssim_scores': ssim_scores,\n",
    "                            'iterations': iterations,\n",
    "                            'success': success\n",
    "                            }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dictionary to file\n",
    "with open('./stats/alexnet_stats.json', 'w') as f:\n",
    "    json.dump(stats_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary from file\n",
    "with open('./stats/alexnet_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"alexnet\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(stats['confidences_pert'])/len(stats['confidences_pert']):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(stats['l2_norms'])/len(stats['l2_norms']):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(stats['ssim_scores'])/len(stats['ssim_scores']):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {sum(stats['iterations'])/len(stats['iterations']):.4f}\")\n",
    "print(f\"Mean success for {model_name}: {sum(stats['success'])/len(stats['success']):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows results after excluding failed attacks\n",
    "\n",
    "with open('./stats/alexnet_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"alexnet\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "success_indices = [i for i, success in enumerate(stats['success']) if success]\n",
    "\n",
    "success_confidences = [stats['confidences_pert'][i] for i in success_indices]\n",
    "success_l2_norms = [stats['l2_norms'][i] for i in success_indices]\n",
    "success_ssim_scores = [stats['ssim_scores'][i] for i in success_indices]\n",
    "success_iterations = [stats['iterations'][i] for i in success_indices]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(success_confidences) / len(success_confidences):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(success_l2_norms) / len(success_l2_norms):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(success_ssim_scores) / len(success_ssim_scores):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {math.ceil(sum(success_iterations) / len(success_iterations))}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EfficientNetV2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "stats_dict = {}\n",
    "stats_dict_temp = {}\n",
    "\n",
    "# Storing true labels in an array\n",
    "filename = './data/LOC_val_solution/ILSVRC2012_val_true_label_indices.txt'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    true_labels = []\n",
    "    for line in file:\n",
    "        true_labels.append(int(line.strip()))\n",
    "\n",
    "net = models.efficientnet_v2_s(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "img_dir = './data/ILSVRC2012_img_val/'\n",
    "\n",
    "files = sorted(os.listdir(img_dir)) # Use this line if you want to run for the whole dataset\n",
    "\n",
    "# Select only the specific file names \n",
    "# [1000:] = start from 1000 and end at file end, \n",
    "# [:1000] = run from start till 1000th file\n",
    "# [65:36000] = run from 66th file to 36000th file\n",
    "\n",
    "# files = os.listdir(img_dir)[36287:] #Change the value inside [] according to need\n",
    "\n",
    "confidences_pert = []\n",
    "l2_norms = []\n",
    "ssim_scores = []\n",
    "index = 0\n",
    "iterations = []\n",
    "success = []\n",
    "\n",
    "for filename in files:\n",
    "    \n",
    "    # Create the 'perturbed_images' directory if it doesn't exist\n",
    "    if not os.path.exists('./data/perturbed_images_efficientnet'):\n",
    "        os.makedirs('./data/perturbed_images_efficientnet')\n",
    "\n",
    "    # Load the image\n",
    "    im_orig = Image.open(os.path.join(img_dir, filename)).convert('RGB')\n",
    "    # im_orig = Image.open(os.path.join(img_dir, filename))\n",
    "    # if im_orig.mode == 'L': # grayscale image\n",
    "    #     im_orig = im_orig.convert('RGB')\n",
    "\n",
    "    mean = [ 0.485, 0.456, 0.406 ]\n",
    "    std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "    # Apply the transformation pipeline\n",
    "    im = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = mean,\n",
    "                             std = std)])(im_orig)\n",
    "    \n",
    "    # randomly select target class\n",
    "    target = np.random.choice([i for i in range(1000) if i != true_labels[index]])\n",
    "\n",
    "    # Perform the targeted deepfool attack\n",
    "    r, loop_i, label_orig, label_pert, pert_image, confidences, confidence_pert, l2_norm, ssim_value, successful = deepfool_targeted(im, net, target)\n",
    "\n",
    "    confidences_pert.append(confidence_pert)\n",
    "    l2_norms.append(l2_norm)\n",
    "    ssim_scores.append(ssim_value)\n",
    "    iterations.append(loop_i)\n",
    "    success.append(successful)\n",
    "\n",
    "    labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "    \n",
    "    # Get the string label for the original and perturbed images\n",
    "    str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "    str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "    def clip_tensor(A, minv, maxv):\n",
    "        A = torch.max(A, minv*torch.ones(A.shape))\n",
    "        A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "        return A\n",
    "\n",
    "    clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "    tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "    transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "    transforms.Lambda(clip),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(224)])\n",
    "\n",
    "    # Clip the perturbed image and convert it back to a PIL image\n",
    "    pert_image = tf(pert_image.cpu()[0])\n",
    "\n",
    "    # Save the perturbed image\n",
    "    pert_image.save(os.path.join('./data/perturbed_images_efficientnet', filename))\n",
    "    \n",
    "    # Print the labels of the original and perturbed images\n",
    "    print(f\"{filename}: Original label = {str_label_orig}, Perturbed label = {str_label_pert}\")\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "    # Add model name as key and corresponding values as values in stats_dict_temp\n",
    "    # Update test_dict with the new values for alexnet\n",
    "    # Check if 'efficientnet' is already in stats_dict_temp\n",
    "    if 'efficientnet' in stats_dict_temp:\n",
    "        # If it is, update the existing dictionary for 'efficientnet'\n",
    "        stats_dict_temp['efficientnet'].update({\n",
    "            'confidences_pert': stats_dict_temp['efficientnet'].get('confidences_pert', []) + [confidence_pert],\n",
    "            'l2_norms': stats_dict_temp['efficientnet'].get('l2_norms', []) + [l2_norm],\n",
    "            'ssim_scores': stats_dict_temp['efficientnet'].get('ssim_scores', []) + [ssim_value],\n",
    "            'iterations': stats_dict_temp['efficientnet'].get('iterations', []) + [loop_i],\n",
    "            'success': stats_dict_temp['efficientnet'].get('success', []) + [successful]\n",
    "        })\n",
    "    else:\n",
    "        # If it isn't, add a new dictionary for 'efficientnet'\n",
    "        stats_dict_temp['efficientnet'] = {\n",
    "            'confidences_pert': [confidence_pert],\n",
    "            'l2_norms': [l2_norm],\n",
    "            'ssim_scores': [ssim_value],\n",
    "            'iterations': [loop_i],\n",
    "            'success': [successful]\n",
    "        }\n",
    "    \n",
    "    with open('./stats/efficientnet_stats_temp.json', 'w') as f:\n",
    "        json.dump(stats_dict_temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time =  time.process_time() - start\n",
    "print(\"Total Time: \",time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model name as key and corresponding values as values in stats_dict\n",
    "stats_dict.update({'efficientnet': {\n",
    "                            'confidences_pert': confidences_pert,\n",
    "                            'l2_norms': l2_norms,\n",
    "                            'ssim_scores': ssim_scores,\n",
    "                            'iterations': iterations,\n",
    "                            'success': success\n",
    "                            }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dictionary to file\n",
    "with open('./stats/efficientnet_stats.json', 'w') as f:\n",
    "    json.dump(stats_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary from file\n",
    "with open('./stats/efficientnet_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"efficientnet\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(stats['confidences_pert'])/len(stats['confidences_pert']):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(stats['l2_norms'])/len(stats['l2_norms']):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(stats['ssim_scores'])/len(stats['ssim_scores']):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {sum(stats['iterations'])/len(stats['iterations']):.4f}\")\n",
    "print(f\"Mean success for {model_name}: {sum(stats['success'])/len(stats['success']):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows results after excluding failed attacks\n",
    "\n",
    "with open('./stats/efficientnet_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"efficientnet\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "success_indices = [i for i, success in enumerate(stats['success']) if success]\n",
    "\n",
    "success_confidences = [stats['confidences_pert'][i] for i in success_indices]\n",
    "success_l2_norms = [stats['l2_norms'][i] for i in success_indices]\n",
    "success_ssim_scores = [stats['ssim_scores'][i] for i in success_indices]\n",
    "success_iterations = [stats['iterations'][i] for i in success_indices]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(success_confidences) / len(success_confidences):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(success_l2_norms) / len(success_l2_norms):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(success_ssim_scores) / len(success_ssim_scores):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {math.ceil(sum(success_iterations) / len(success_iterations))}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GoogLeNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "stats_dict = {}\n",
    "stats_dict_temp = {}\n",
    "\n",
    "# Storing true labels in an array\n",
    "filename = './data/LOC_val_solution/ILSVRC2012_val_true_label_indices.txt'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    true_labels = []\n",
    "    for line in file:\n",
    "        true_labels.append(int(line.strip()))\n",
    "\n",
    "net = models.googlenet(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "img_dir = './data/ILSVRC2012_img_val/'\n",
    "\n",
    "files = sorted(os.listdir(img_dir)) # Use this line if you want to run for the whole dataset\n",
    "\n",
    "# Select only the specific file names \n",
    "# [1000:] = start from 1000 and end at file end, \n",
    "# [:1000] = run from start till 1000th file\n",
    "# [65:36000] = run from 66th file to 36000th file\n",
    "\n",
    "# files = sorted(os.listdir(img_dir))[19876:] #Change the value inside [] according to need\n",
    "\n",
    "confidences_pert = []\n",
    "l2_norms = []\n",
    "ssim_scores = []\n",
    "index = 0\n",
    "iterations = []\n",
    "success = []\n",
    "\n",
    "for filename in files:\n",
    "    \n",
    "    # Create the 'perturbed_images' directory if it doesn't exist\n",
    "    if not os.path.exists('./data/perturbed_images_googlenet'):\n",
    "        os.makedirs('./data/perturbed_images_googlenet')\n",
    "\n",
    "    # Load the image\n",
    "    im_orig = Image.open(os.path.join(img_dir, filename)).convert('RGB')\n",
    "    # im_orig = Image.open(os.path.join(img_dir, filename))\n",
    "    # if im_orig.mode == 'L': # grayscale image\n",
    "    #     im_orig = im_orig.convert('RGB')\n",
    "\n",
    "    mean = [ 0.485, 0.456, 0.406 ]\n",
    "    std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "    # Apply the transformation pipeline\n",
    "    im = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = mean,\n",
    "                             std = std)])(im_orig)\n",
    "    \n",
    "    # randomly select target class\n",
    "    target = np.random.choice([i for i in range(1000) if i != true_labels[index]])\n",
    "\n",
    "    # Perform the targeted deepfool attack\n",
    "    r, loop_i, label_orig, label_pert, pert_image, confidences, confidence_pert, l2_norm, ssim_value, successful = deepfool_targeted(im, net, target)\n",
    "\n",
    "    confidences_pert.append(confidence_pert)\n",
    "    l2_norms.append(l2_norm)\n",
    "    ssim_scores.append(ssim_value)\n",
    "    iterations.append(loop_i)\n",
    "    success.append(successful)\n",
    "\n",
    "    labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "    \n",
    "    # Get the string label for the original and perturbed images\n",
    "    str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "    str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "    def clip_tensor(A, minv, maxv):\n",
    "        A = torch.max(A, minv*torch.ones(A.shape))\n",
    "        A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "        return A\n",
    "\n",
    "    clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "    tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "    transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "    transforms.Lambda(clip),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(224)])\n",
    "\n",
    "    # Clip the perturbed image and convert it back to a PIL image\n",
    "    pert_image = tf(pert_image.cpu()[0])\n",
    "\n",
    "    # Save the perturbed image\n",
    "    pert_image.save(os.path.join('./data/perturbed_images_googlenet', filename))\n",
    "    \n",
    "    # Print the labels of the original and perturbed images\n",
    "    print(f\"{filename}: Original label = {str_label_orig}, Perturbed label = {str_label_pert}\")\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "    # Add model name as key and corresponding values as values in stats_dict_temp\n",
    "    # Update test_dict with the new values for alexnet\n",
    "    # Check if 'googlenet' is already in stats_dict_temp\n",
    "    if 'googlenet' in stats_dict_temp:\n",
    "        # If it is, update the existing dictionary for 'googlenet'\n",
    "        stats_dict_temp['googlenet'].update({\n",
    "            'confidences_pert': stats_dict_temp['googlenet'].get('confidences_pert', []) + [confidence_pert],\n",
    "            'l2_norms': stats_dict_temp['googlenet'].get('l2_norms', []) + [l2_norm],\n",
    "            'ssim_scores': stats_dict_temp['googlenet'].get('ssim_scores', []) + [ssim_value],\n",
    "            'iterations': stats_dict_temp['googlenet'].get('iterations', []) + [loop_i],\n",
    "            'success': stats_dict_temp['googlenet'].get('success', []) + [successful]\n",
    "        })\n",
    "    else:\n",
    "        # If it isn't, add a new dictionary for 'googlenet'\n",
    "        stats_dict_temp['googlenet'] = {\n",
    "            'confidences_pert': [confidence_pert],\n",
    "            'l2_norms': [l2_norm],\n",
    "            'ssim_scores': [ssim_value],\n",
    "            'iterations': [loop_i],\n",
    "            'success': [successful]\n",
    "        }\n",
    "    \n",
    "    with open('./stats/googlenet_stats_temp.json', 'w') as f:\n",
    "        json.dump(stats_dict_temp, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model name as key and corresponding values as values in stats_dict\n",
    "stats_dict.update({'googlenet': {\n",
    "                            'confidences_pert': confidences_pert,\n",
    "                            'l2_norms': l2_norms,\n",
    "                            'ssim_scores': ssim_scores,\n",
    "                            'iterations': iterations,\n",
    "                            'success': success\n",
    "                            }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dictionary to file\n",
    "with open('./stats/googlenet_stats.json', 'w') as f:\n",
    "    json.dump(stats_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary from file\n",
    "with open('./stats/googlenet_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"googlenet\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(stats['confidences_pert'])/len(stats['confidences_pert']):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(stats['l2_norms'])/len(stats['l2_norms']):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(stats['ssim_scores'])/len(stats['ssim_scores']):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {sum(stats['iterations'])/len(stats['iterations']):.4f}\")\n",
    "print(f\"Mean success for {model_name}: {sum(stats['success'])/len(stats['success']):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows results after excluding failed attacks\n",
    "\n",
    "with open('./stats/googlenet_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"googlenet\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "success_indices = [i for i, success in enumerate(stats['success']) if success]\n",
    "\n",
    "success_confidences = [stats['confidences_pert'][i] for i in success_indices]\n",
    "success_l2_norms = [stats['l2_norms'][i] for i in success_indices]\n",
    "success_ssim_scores = [stats['ssim_scores'][i] for i in success_indices]\n",
    "success_iterations = [stats['iterations'][i] for i in success_indices]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(success_confidences) / len(success_confidences):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(success_l2_norms) / len(success_l2_norms):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(success_ssim_scores) / len(success_ssim_scores):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {math.ceil(sum(success_iterations) / len(success_iterations))}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **InceptionV3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "stats_dict = {}\n",
    "stats_dict_temp = {}\n",
    "\n",
    "# Storing true labels in an array\n",
    "filename = './data/LOC_val_solution/ILSVRC2012_val_true_label_indices.txt'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    true_labels = []\n",
    "    for line in file:\n",
    "        true_labels.append(int(line.strip()))\n",
    "\n",
    "net = models.inception_v3(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "img_dir = './data/ILSVRC2012_img_val/'\n",
    "\n",
    "# files = sorted(os.listdir(img_dir)) # Use this line if you want to run for the whole dataset\n",
    "\n",
    "# Select only the specific file names \n",
    "# [1000:] = start from 1000 and end at file end, \n",
    "# [:1000] = run from start till 1000th file\n",
    "# [65:36000] = run from 66th file to 36000th file\n",
    "\n",
    "files = sorted(os.listdir(img_dir))[36306:] #Change the value inside [] according to need\n",
    "\n",
    "confidences_pert = []\n",
    "l2_norms = []\n",
    "ssim_scores = []\n",
    "index = 0\n",
    "iterations = []\n",
    "success = []\n",
    "\n",
    "for filename in files:\n",
    "    \n",
    "    # Create the 'perturbed_images' directory if it doesn't exist\n",
    "    if not os.path.exists('./data/perturbed_images_inceptionV3'):\n",
    "        os.makedirs('./data/perturbed_images_inceptionV3')\n",
    "\n",
    "    # Load the image\n",
    "    im_orig = Image.open(os.path.join(img_dir, filename)).convert('RGB')\n",
    "    # im_orig = Image.open(os.path.join(img_dir, filename))\n",
    "    # if im_orig.mode == 'L': # grayscale image\n",
    "    #     im_orig = im_orig.convert('RGB')\n",
    "\n",
    "    mean = [ 0.485, 0.456, 0.406 ]\n",
    "    std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "    # Apply the transformation pipeline\n",
    "    im = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = mean,\n",
    "                             std = std)])(im_orig)\n",
    "    \n",
    "    # randomly select target class\n",
    "    target = np.random.choice([i for i in range(1000) if i != true_labels[index]])\n",
    "\n",
    "    # Perform the targeted deepfool attack\n",
    "    r, loop_i, label_orig, label_pert, pert_image, confidences, confidence_pert, l2_norm, ssim_value, successful = deepfool_targeted(im, net, target)\n",
    "\n",
    "    confidences_pert.append(confidence_pert)\n",
    "    l2_norms.append(l2_norm)\n",
    "    ssim_scores.append(ssim_value)\n",
    "    iterations.append(loop_i)\n",
    "    success.append(successful)\n",
    "\n",
    "    labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "    \n",
    "    # Get the string label for the original and perturbed images\n",
    "    str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "    str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "    def clip_tensor(A, minv, maxv):\n",
    "        A = torch.max(A, minv*torch.ones(A.shape))\n",
    "        A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "        return A\n",
    "\n",
    "    clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "    tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "    transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "    transforms.Lambda(clip),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(224)])\n",
    "\n",
    "    # Clip the perturbed image and convert it back to a PIL image\n",
    "    pert_image = tf(pert_image.cpu()[0])\n",
    "\n",
    "    # Save the perturbed image\n",
    "    pert_image.save(os.path.join('./data/perturbed_images_inceptionV3', filename))\n",
    "    \n",
    "    # Print the labels of the original and perturbed images\n",
    "    print(f\"{filename}: Original label = {str_label_orig}, Perturbed label = {str_label_pert}\")\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "    # Add model name as key and corresponding values as values in stats_dict_temp\n",
    "    # Update test_dict with the new values for alexnet\n",
    "    # Check if 'inceptionV3' is already in stats_dict_temp\n",
    "    if 'inceptionV3' in stats_dict_temp:\n",
    "        # If it is, update the existing dictionary for 'inceptionV3'\n",
    "        stats_dict_temp['inceptionV3'].update({\n",
    "            'confidences_pert': stats_dict_temp['inceptionV3'].get('confidences_pert', []) + [confidence_pert],\n",
    "            'l2_norms': stats_dict_temp['inceptionV3'].get('l2_norms', []) + [l2_norm],\n",
    "            'ssim_scores': stats_dict_temp['inceptionV3'].get('ssim_scores', []) + [ssim_value],\n",
    "            'iterations': stats_dict_temp['inceptionV3'].get('iterations', []) + [loop_i],\n",
    "            'success': stats_dict_temp['inceptionV3'].get('success', []) + [successful]\n",
    "        })\n",
    "    else:\n",
    "        # If it isn't, add a new dictionary for 'inceptionV3'\n",
    "        stats_dict_temp['inceptionV3'] = {\n",
    "            'confidences_pert': [confidence_pert],\n",
    "            'l2_norms': [l2_norm],\n",
    "            'ssim_scores': [ssim_value],\n",
    "            'iterations': [loop_i],\n",
    "            'success': [successful]\n",
    "        }\n",
    "    \n",
    "    with open('./stats/inceptionV3_stats_temp.json', 'w') as f:\n",
    "        json.dump(stats_dict_temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model name as key and corresponding values as values in stats_dict\n",
    "stats_dict.update({'inceptionV3': {\n",
    "                            'confidences_pert': confidences_pert,\n",
    "                            'l2_norms': l2_norms,\n",
    "                            'ssim_scores': ssim_scores,\n",
    "                            'iterations': iterations,\n",
    "                            'success': success\n",
    "                            }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dictionary to file\n",
    "with open('./stats/inceptionV3_stats.json', 'w') as f:\n",
    "    json.dump(stats_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary from file\n",
    "with open('./stats/inceptionV3_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"inceptionV3\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(stats['confidences_pert'])/len(stats['confidences_pert']):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(stats['l2_norms'])/len(stats['l2_norms']):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(stats['ssim_scores'])/len(stats['ssim_scores']):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {sum(stats['iterations'])/len(stats['iterations']):.4f}\")\n",
    "print(f\"Mean success for {model_name}: {sum(stats['success'])/len(stats['success']):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows results after excluding failed attacks\n",
    "\n",
    "with open('./stats/inceptionV3_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"inceptionV3\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "success_indices = [i for i, success in enumerate(stats['success']) if success]\n",
    "\n",
    "success_confidences = [stats['confidences_pert'][i] for i in success_indices]\n",
    "success_l2_norms = [stats['l2_norms'][i] for i in success_indices]\n",
    "success_ssim_scores = [stats['ssim_scores'][i] for i in success_indices]\n",
    "success_iterations = [stats['iterations'][i] for i in success_indices]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(success_confidences) / len(success_confidences):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(success_l2_norms) / len(success_l2_norms):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(success_ssim_scores) / len(success_ssim_scores):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {math.ceil(sum(success_iterations) / len(success_iterations))}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VisionTransformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.process_time()\n",
    "stats_dict = {}\n",
    "stats_dict_temp = {}\n",
    "\n",
    "# Storing true labels in an array\n",
    "filename = './data/LOC_val_solution/ILSVRC2012_val_true_label_indices.txt'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    true_labels = []\n",
    "    for line in file:\n",
    "        true_labels.append(int(line.strip()))\n",
    "\n",
    "net = models.vit_b_16(pretrained=True)\n",
    "\n",
    "# Switch to evaluation mode\n",
    "net.eval()\n",
    "\n",
    "img_dir = './data/ILSVRC2012_img_val/'\n",
    "\n",
    "files = sorted(os.listdir(img_dir)) # Use this line if you want to run for the whole dataset\n",
    "\n",
    "# Select only the specific file names \n",
    "# [1000:] = start from 1000 and end at file end, \n",
    "# [:1000] = run from start till 1000th file\n",
    "# [65:36000] = run from 66th file to 36000th file\n",
    "\n",
    "# files = sorted(os.listdir(img_dir))[19876:] #Change the value inside [] according to need\n",
    "\n",
    "confidences_pert = []\n",
    "l2_norms = []\n",
    "ssim_scores = []\n",
    "index = 0\n",
    "iterations = []\n",
    "success = []\n",
    "\n",
    "for filename in files:\n",
    "    \n",
    "    # Create the 'perturbed_images' directory if it doesn't exist\n",
    "    if not os.path.exists('./data/perturbed_images_visionTransformer'):\n",
    "        os.makedirs('./data/perturbed_images_visionTransformer')\n",
    "\n",
    "    # Load the image\n",
    "    im_orig = Image.open(os.path.join(img_dir, filename)).convert('RGB')\n",
    "    # im_orig = Image.open(os.path.join(img_dir, filename))\n",
    "    # if im_orig.mode == 'L': # grayscale image\n",
    "    #     im_orig = im_orig.convert('RGB')\n",
    "\n",
    "    mean = [ 0.485, 0.456, 0.406 ]\n",
    "    std = [ 0.229, 0.224, 0.225 ]\n",
    "\n",
    "    # Apply the transformation pipeline\n",
    "    im = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean = mean,\n",
    "                             std = std)])(im_orig)\n",
    "    \n",
    "    # randomly select target class\n",
    "    target = np.random.choice([i for i in range(1000) if i != true_labels[index]])\n",
    "\n",
    "    # Perform the targeted deepfool attack\n",
    "    r, loop_i, label_orig, label_pert, pert_image, confidences, confidence_pert, l2_norm, ssim_value, successful = deepfool_targeted(im, net, target)\n",
    "\n",
    "    confidences_pert.append(confidence_pert)\n",
    "    l2_norms.append(l2_norm)\n",
    "    ssim_scores.append(ssim_value)\n",
    "    iterations.append(loop_i)\n",
    "    success.append(successful)\n",
    "\n",
    "    labels = open(os.path.join('synset_words.txt'), 'r').read().split('\\n')\n",
    "    \n",
    "    # Get the string label for the original and perturbed images\n",
    "    str_label_orig = labels[np.int_(label_orig)].split(',')[0]\n",
    "    str_label_pert = labels[np.int_(label_pert)].split(',')[0]\n",
    "\n",
    "    def clip_tensor(A, minv, maxv):\n",
    "        A = torch.max(A, minv*torch.ones(A.shape))\n",
    "        A = torch.min(A, maxv*torch.ones(A.shape))\n",
    "        return A\n",
    "\n",
    "    clip = lambda x: clip_tensor(x, 0, 255)\n",
    "\n",
    "    tf = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], std=list(map(lambda x: 1 / x, std))),\n",
    "    transforms.Normalize(mean=list(map(lambda x: -x, mean)), std=[1, 1, 1]),\n",
    "    transforms.Lambda(clip),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.CenterCrop(224)])\n",
    "\n",
    "    # Clip the perturbed image and convert it back to a PIL image\n",
    "    pert_image = tf(pert_image.cpu()[0])\n",
    "\n",
    "    # Save the perturbed image\n",
    "    pert_image.save(os.path.join('./data/perturbed_images_visionTransformer', filename))\n",
    "    \n",
    "    # Print the labels of the original and perturbed images\n",
    "    print(f\"{filename}: Original label = {str_label_orig}, Perturbed label = {str_label_pert}\")\n",
    "    \n",
    "    index += 1\n",
    "\n",
    "    # Add model name as key and corresponding values as values in stats_dict\n",
    "    # Update test_dict with the new values for alexnet\n",
    "    # Check if 'visionTransformer' is already in stats_dict_temp\n",
    "    if 'visionTransformer' in stats_dict_temp:\n",
    "        # If it is, update the existing dictionary for 'visionTransformer'\n",
    "        stats_dict_temp['visionTransformer'].update({\n",
    "            'confidences_pert': stats_dict_temp['visionTransformer'].get('confidences_pert', []) + [confidence_pert],\n",
    "            'l2_norms': stats_dict_temp['visionTransformer'].get('l2_norms', []) + [l2_norm],\n",
    "            'ssim_scores': stats_dict_temp['visionTransformer'].get('ssim_scores', []) + [ssim_value],\n",
    "            'iterations': stats_dict_temp['visionTransformer'].get('iterations', []) + [loop_i],\n",
    "            'success': stats_dict_temp['visionTransformer'].get('success', []) + [successful]\n",
    "        })\n",
    "    else:\n",
    "        # If it isn't, add a new dictionary for 'visionTransformer'\n",
    "        stats_dict_temp['visionTransformer'] = {\n",
    "            'confidences_pert': [confidence_pert],\n",
    "            'l2_norms': [l2_norm],\n",
    "            'ssim_scores': [ssim_value],\n",
    "            'iterations': [loop_i],\n",
    "            'success': [successful]\n",
    "        }\n",
    "    \n",
    "    with open('./stats/visionTransformer_stats_temp_.json', 'w') as f:\n",
    "        json.dump(stats_dict_temp, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time =  time.process_time() - start\n",
    "print(\"Total Time: \",time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model name as key and corresponding values as values in stats_dict\n",
    "stats_dict.update({'visionTransformer': {\n",
    "                            'confidences_pert': confidences_pert,\n",
    "                            'l2_norms': l2_norms,\n",
    "                            'ssim_scores': ssim_scores,\n",
    "                            'iterations': iterations,\n",
    "                            'success': success\n",
    "                            }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dictionary to file\n",
    "with open('./stats/visionTransformer_stats.json', 'w') as f:\n",
    "    json.dump(stats_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary from file\n",
    "with open('./stats/visionTransformer_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"visionTransformer\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(stats['confidences_pert'])/len(stats['confidences_pert']):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(stats['l2_norms'])/len(stats['l2_norms']):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(stats['ssim_scores'])/len(stats['ssim_scores']):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {sum(stats['iterations'])/len(stats['iterations']):.4f}\")\n",
    "print(f\"Mean success for {model_name}: {sum(stats['success'])/len(stats['success']):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows results after excluding failed attacks\n",
    "\n",
    "with open('./stats/visionTransformer_stats.json', 'r') as f:\n",
    "    stats_dict = json.load(f)\n",
    "\n",
    "model_name = \"visionTransformer\"\n",
    "stats = stats_dict[model_name]\n",
    "\n",
    "success_indices = [i for i, success in enumerate(stats['success']) if success]\n",
    "\n",
    "success_confidences = [stats['confidences_pert'][i] for i in success_indices]\n",
    "success_l2_norms = [stats['l2_norms'][i] for i in success_indices]\n",
    "success_ssim_scores = [stats['ssim_scores'][i] for i in success_indices]\n",
    "success_iterations = [stats['iterations'][i] for i in success_indices]\n",
    "\n",
    "print(f\"Mean Confidence for {model_name}: {sum(success_confidences) / len(success_confidences):.4f}\")\n",
    "print(f\"Mean l2_norm for {model_name}: {sum(success_l2_norms) / len(success_l2_norms):.4f}%\")\n",
    "print(f\"Mean SSIM score for {model_name}: {sum(success_ssim_scores) / len(success_ssim_scores):.4f}\")\n",
    "print(f\"Mean Iterations for {model_name}: {math.ceil(sum(success_iterations) / len(success_iterations))}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
